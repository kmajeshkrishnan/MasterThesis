\chapter{Related Work}\label{chap:relatedwork}
%Give a brief overview of the work relevant for your thesis. 
\section{Self-supervised feature learning}
Self-supervised feature learning is a crucial process that identifies patterns within extensive unlabeled datasets without the need for human-annotated labels. Plenty of research has been done in this field in the recent years. Several methods have been proposed, each with unique mechanisms and varying levels of success.

\subsection{Contrastive learning}
Contrastive learning has gained significant attention for its effectiveness in self-supervised feature learning. One of the seminal works in this area is SimCLR\cite{chen2020simple}, It employs a simple yet robust framework that leverages data augmentations to create positive pairs from the same image and negative pairs from different images. The model uses a contrastive loss to distinguish between these pairs, learning robust representations in the process. On the other hand, MoCo (Momentum Contrast)\cite{he2020momentum} introduces a dynamic dictionary with a momentum encoder. This approach allows the model to maintain a queue of negative samples, effectively reducing memory requirements and improving scalability. Nevertheless, it still requires a substantial number of negative samples to function optimally and necessitates careful tuning of the momentum parameter to balance stability and learning efficiency.

\subsection{Clustering-based feature learning}
Clustering-based feature learning approaches automatically uncover the natural groupings of data within the latent representation space. This clustering process helps in understanding the inherent structure of the data by grouping similar data points together based on learned features. Agglomerative Clustering with Self-supervision\cite{asano2020selflabelling} can capture multi-scale structures and found to be effective for diverse datasets. But found to be computationally expensive and needs careful tuning of the self-supervised task. SwAV\cite{caron2021unsupervised} combines clustering with contrastive learning by swapping assignments between different augmented views of the image. This method is efficient in terms of computational resources and achieves state-of-the-art performance on several benchmarks. But it is sensitive to the choice of hyperparameters.

\subsection{Distillation-based methods}
Distillation-based methods have also shown considerable promise in self-supervised learning. BYOL (Bootstrap Your Own Latent)\cite{grill2020bootstrap} introduces a teacher-student network where the student learns to predict the teacher's representations. Remarkably, BYOL achieves this without using negative samples, simplifying the training process and reducing computational demands. However, it is sensitive to the choice of data augmentations and network architecture, and there is a potential risk of model collapse if not properly tuned. DINO\cite{caron2021emerging}, extends the self-distillation approach to Vision Transformers\cite{dosovitskiy2020image}. DINO captures global image representations effectively without relying on negative samples. It shows strong performance on object detection and segmentation tasks, showcasing the potential of transformers in self-supervised learning.

Unlike these unsupervised representation learning efforts, our research revolves around CutLer\cite{wang2023cut}, which focuses on automatically identifying natural pixel groupings and detecting instances within each image.

\section{Unsupervised object detection and instance segmentation}
If we consider the recent methods for unsupervised object detection semantic segmentation, most of them leverage on self-supervised Vision Transformer(ViT)\cite{dosovitskiy2020image} features. DINO\cite{caron2021emerging} observes that the underlying semantic segmentation of images can be extracted using the saliency maps from the ViT. 

The quality of this segmentation is superior to the existing methods if the image contains only is one instance. The superiority of DINO features to separate foreground and background has been affirmed by later works\cite{engstler2023understanding}. Building on this observation, both LOST \cite{simeoni2021localizing} and TokenCut \cite{wang2022tokencut} utilize DINO features to segment a single salient object from each image. These methods capitalize on the strength of DINO to construct a graph from the features of image patches. Unlike TokenCut and DINO, which can only detect one instance, LOST is capable of finding multiple instances within an image. But it can't be used as a pre-trained model for down stream tasks. But CutLer\cite{wang2023cut} not only can detect multiple instances, the model can be further used as a pretrained model for label-efficient and fully-supervised learning.

FreeSOLO\cite{wang2022freesolo} and the follow up work Exemplar-FreeSOLO\cite{Ishtiak_2023_CVPR} (with its addition of a randomly drawn pool of exemplars used in a contrastive learning loss) generates coarse segmentation masks with low guality and refines it further through self training similar to Cutler. But the poor quality of the coarse maps is a major draw back of this method, where as CutLer masks made by the MaskCut\cite{wang2023cut, wang2022tokencut} algorithm are usually better in quality and quantity than the initial masks used by MaskDistill\cite{vangansbeke2022discovering} and \cite{wang2022freesolo}. Even though Maskdistill produces similar quality masks compared to MaskCut, as it only produces one class agnostic mask per image and MaskCut produces N fixed number of masks per image to use as pseudo labels, MaskCut weighs over Maskdistill in quantity.

As CutLer dominates in most cases, including producing better pseudo ground truth masks, ablility to detect multiple instances, compatibility with various detection architectures, usable as pretrained model for supervised detection, our work would mostly focus on studying and improving the performance of CutLer.

\section{Semi-supervised object detection and instance segmentation}
In the realm of unsupervised semantic segmentation, the challenge of acquiring dense annotations has spurred innovative research efforts. Addressing this constraint, a series of studies \cite{hwang2019segsort,melas2022deep,cho2021picie,wang2022tokencut,hamilton2022unsupervised} have delved into harnessing self-supervised learning techniques to cultivate feature representations capable of supporting segmentation without necessitating extensive annotation efforts. Particularly, the DINO self-supervised model has illuminated that features extracted from its deep layers exhibit noteworthy correlation patterns congruent with genuine semantic labels. This insight has spurred further investigation. Drawing from this, Hamilton et al. introduced STEGO, which employs contrastive loss to distill pre-trained unsupervised visual features into semantic clusters, thus achieving compelling unsupervised segmentation results \cite{hamilton2022unsupervised}. Furthermore, Melas-Kyriazi et al. leverage spectral clustering on deep unsupervised representations, achieving state-of-the-art performance while also capitalizing on the informative features derived from DINO's architecture. Further This growing body of work showcases the potential of self-supervised methods, especially those rooted in DINO, to advance the landscape of unsupervised semantic segmentation \cite{melas2022deep}.
% \todo{see if you included all literature work for Vision Transformer for Semantic Segmentation}
% \todo{ add little bit about Hierarchical Vision Transformer for say swin, satrt with where all hierarchy in transformer have been explored and in semnatic segmentation where all it fanne dout well}
% \todo{Then in the the section say a very little about groupVit and that you would be moving on to that and our ork builds on that }

% \section{Visual Grouping}

% \section{Visual Language Pretraining}
% Vision-language pretraining has emerged as a powerful paradigm, driving advancements in the intersection of vision and language understanding. By jointly modeling visual and textual modalities, these pretrained models have revolutionized a range of vision-language tasks. Vision-language pretraining involves training a joint model on a vast corpus of image and text data. By learning from the paired visual and textual information, the model gains a comprehensive understanding of the cross-modal relationships between images and their associated captions or descriptions. 

% Explain the math and notation.