\chapter{Related Work}\label{chap:relatedwork}
%Give a brief overview of the work relevant for your thesis. 
\section{Semantic Segmentation}
% Semantic segmentation, a task involving the partitioning of images into meaningful segments, necessitates a nuanced understanding of both local and global context. Initially, the advent of Fully Convolutional Networks (FCNs) marked a pivotal moment, employing pixel-level classification to excel on benchmarks like PASCAL VOC and SIFT Flow \cite{long2015fully}. This progress was amplified with the introduction of the Pyramid Scene Parsing Network (PSPNet) \cite{zhao2017pyramid}, further enriching the capabilities of scene comprehension. Modern semantic segmentation models have shifted their focus towards aggregating long-range context in the final feature map. Approaches such as ASPP \cite{chen2017rethinking}, PPM \cite{zhao2017pyramid}, DANet \cite{fu2019dual}, OCNet \cite{yuan2018ocnet}, and CCNet \cite{huang2019ccnet} employ a diverse array of techniques, such as atrous convolutions and non-local blocks, to capture extensive context. More recently, approaches like SETR \cite{zheng2021rethinking} and Segmenter \cite{strudel2021segmenter} have swapped traditional convolutional backbones for Vision Transformers (ViTs) \cite{dosovitskiy2020image}, aligning with the trend to integrate transformer architectures into segmentation tasks. \\
% As the landscape evolved, Vision Transformers (ViTs) emerged as a potential alternative, capturing long-range dependencies in images \cite{dosovitskiy2020image}\cite{strudel2021segmenter}. Another research avenue in the pursuit of leveraging ViTs for semantic segmentation, was the integration spatial attention mechanisms, refining segmentation outcomes. A pivotal milestone along this trajectory was the Swin Transformer (SwiN) proposed by Liu et al. \cite{liu2021swin}. SwiN's innovative hierarchical structure efficiently captured both local intricacies and global context. By strategically attending to image patches, SwiN excelled in understanding complex structures and intricate details. This marked a dynamic evolution, blending attention mechanisms with transformer architectures to address segmentation challenges. However, SwiN's efficiency with high-resolution inputs remains a subject of concern.\\
% In this continuum, MaskFormer \cite{cheng2021per} introduced a unified paradigm for both instance and semantic segmentation, leveraging mask classification to outperform previous benchmarks.\\
% Despite these strides, these models heavily rely on labor-intensive pixel-level annotations, limiting scalability. Additionally, they grapple with challenges posed by diverse categories and domain shifts. Acknowledging these limitations, the research committee's focus shifted towards harnessing weak signals, acknowledging the impracticality of dense pixel-level annotations and the intricate domain variations.
In the field of semantic segmentation, the task of dividing images into meaningful segments has evolved significantly over time. Initially, Fully Convolutional Networks (FCNs) were a major breakthrough, as they excelled at pixel-level classification, achieving impressive results on benchmarks like PASCAL VOC and SIFT Flow \cite{long2015fully}. This marked the starting point of progress in the field.

The introduction of the Pyramid Scene Parsing Network (PSPNet) \cite{zhao2017pyramid} further advanced the understanding of scene comprehension. However, the real shift in focus came with the need to capture long-range context in feature maps.

Various approaches emerged to address this need. Techniques like Atrous Spatial Pyramid Pooling (ASPP) \cite{chen2017rethinking}, Pyramid Pooling Module (PPM) \cite{zhao2017pyramid}, Dual Attention Network (DANet) \cite{fu2019dual}, Object-Context Network (OCNet) \cite{yuan2018ocnet}, and Criss-Cross Network (CCNet) \cite{huang2019ccnet} used methods such as atrous convolutions and non-local blocks to capture extensive context.

More recently, there was a significant shift towards adopting Vision Transformers (ViTs) \cite{dosovitskiy2020image} as backbones. Models like SETR \cite{zheng2021rethinking} and Segmenter \cite{strudel2021segmenter} replaced traditional convolutional backbones with ViTs, aligning with the broader trend of integrating transformer architectures into segmentation tasks. ViTs were particularly attractive for their ability to capture long-range dependencies in images \cite{dosovitskiy2020image}\cite{strudel2021segmenter}.

Within the ViT landscape, the integration of spatial attention mechanisms became a key research direction, refining segmentation outcomes. This culminated in the development of the Swin Transformer (Swin) \cite{liu2021swin}, which efficiently captured both local intricacies and global context by strategically attending to image patches. This marked a dynamic evolution, blending attention mechanisms with transformer architectures to address segmentation challenges.

In this chronological progression, MaskFormer \cite{cheng2021per} introduced a unified paradigm for both instance and semantic segmentation, leveraging mask classification to achieve performance levels that surpassed previous benchmarks.

However, despite these strides in the field, these advanced models still heavily depend on labor-intensive pixel-level annotations, which limits their scalability. Moreover, they face challenges posed by diverse categories and domain shifts. Recognizing these limitations, the research focus has shifted towards harnessing weak signals, acknowledging the impracticality of dense pixel-level annotations and domain variations.
\subsection{Weakly Supervised Segmentation}
Methods for weakly supervised segmentation offer an approach to decrease the dependency on fully annotated data by harnessing less precise forms of guidance. These guidance signals can originate from a variety of annotation types, such as bounding boxes (Papandreou et al., 2015; Dai et al., 2015; Khoreva et al., 2017), scribbles (Lin et al., 2016; Vernaza et al., 2017), points (Bearman et al., 2016), and textual information (Ghiasi et al., 2022; Zhou et al., 2022; Zabari et al., 2021; Xu et al., 2022a; Xu et al., 2022b). Nevertheless, effectively harnessing these signals with weak supervision and achieving adaptability across diverse domains continues to pose a challenge.

These techniques primarily utilize sparsely labeled data to produce comprehensive labelings. For example, bounding box annotations serve as an alternative or supplementary form of supervision for training neural networks in semantic segmentation. This is done by employing region proposal techniques \cite{uijlings2013selective} to create candidate segmentation masks, which are then used as supervision during the convolutional network's training process. Similarly, scribbling, which offers a user-friendly way of interaction, is used to supervise the network's training. These algorithms often use graphical models to propagate information from marked pixels to unmarked ones.

More recently, the use of language data as auxiliary weak supervision has gained popularity. This approach offers several advantages, such as requiring less labeling effort, making it a more cost-effective option. Additionally, language data, particularly image captions, is readily available and easier to obtain compared to annotations like bounding boxes or masks. Another benefit is that language data provides a larger vocabulary size, making it more versatile and adaptable. For example, captions can include novel class names, attributes, and descriptions of object movements, extending beyond predefined base categories. Incorporating captions into the training process has proven to be highly beneficial in enhancing the scalability of models in weakly supervised segmentation.
% Weakly supervised segmentation methods alleviate the dependency on fully labeled data by leveraging weak supervision signals. These signals can come from various sources such as bounding box\cite{papandreou2015weakly}\cite{dai2015boxsup}\cite{khoreva2017simple}, scribble\cite{lin2016scribblesup} \cite{vernaza2017learning}, point \cite{bearman2016s} and text\cite{ghiasi2022scaling}\cite{zhou2022extract}\cite{zabari2021semantic}\cite{xu2022simple}\cite{xu2022groupvit}.However, challenges remain in effectively utilizing weak supervision signals and generalizing to diverse domains.These methods predominantly use cheaply obtained sparse image labelings to produce guessed dense labelings. For instance, bounding box annotations are used as an alternative or extra source of supervision to train neural networks for semantic segmentation by using region proposal methods \cite{uijlings2013selective} to generate candidate segmentation masks which are then used as supervision to train the convolutional network. Similary, an user-friendly way of interaction - scribbling is also used to supervised the training network. These algorithms use
% graphical model to propogate information from marked pixels to unmarked pixels. Recently, usage of language data as auxiliary weak supervision has gained quite a popularity. It has many benefits such as it requires less labeling effort and thus
% is more cost-friendly.Moreover, The visual-related language data, like image captions, is widely available and more affordable to obtain compared to the box or mask annotations. Secondly, 
% Language data provides a larger vocabulary size and thus is more extendable and general. For example, words in captions are not limited to the pre-defined base categories.It may contain novel class names, attributes, and motions
% of objects. Incorporating captions in training is proved to be extremely useful in helping improve the modelsâ€™ scalability

\section{Open Vocabulary Semantic Segmenatation}
Zero-shot segmentation represents a groundbreaking computer vision technique that enables object segmentation without the need for specific class training. It achieves this by leveraging auxiliary information and semantic transfer to segment novel object classes. This approach relies on the integration of Visual-Language Embeddings and Semantic Transfer. Recent advancements in Visual-Language Models (VLMs) such as CLIP and ALIGN have demonstrated remarkable classification performance by bridging the gap between visual and textual data through extensive dataset training \cite{radford2021learning, jia2021scaling}. The creation of a shared semantic space empowers models to generalize their understanding and effectively segment objects from classes they have never encountered before.

The success of these developments has spurred ongoing research to adapt these strategies to segmentation tasks. For instance, LSeg aligns VLM text embeddings with dense image embeddings, harnessing the generalization capabilities of VLMs \cite{Li2022LanguagedrivenSS}. Another approach, OpenSeg, enhances state-of-the-art open-vocabulary classification models by incorporating mid-level visual groupings into concise segmentation masks \cite{ghiasi2022scaling}. MaskCLIP+ adapts a frozen CLIP model and utilizes pseudo per-pixel labeling for semantic segmentation \cite{zhou2022extract}. GroupViT, OVSeg, and SegCLIP tackle open-vocabulary segmentation using weak supervision alone \cite{xu2022groupvit, liang2023open, luo2023segclip}. They enhance the model's robustness by primarily training it on text data without relying on masks. Another innovative approach, Patch Aligned Contrastive Loss(PACL), aims to improve CLIP for segmentation \cite{mukhoti2023open}. While weak supervision training holds promise in addressing long-standing challenges, achieving the same level of efficiency as mask-trained models for complex scene understanding remains an ongoing endeavor.

\section{Unsupervised Semantic Segmentation}
In the realm of unsupervised semantic segmentation, the challenge of acquiring dense annotations has spurred innovative research efforts. Addressing this constraint, a series of studies \cite{hwang2019segsort,melas2022deep,cho2021picie,wang2022tokencut,hamilton2022unsupervised} have delved into harnessing self-supervised learning techniques to cultivate feature representations capable of supporting segmentation without necessitating extensive annotation efforts. Particularly, the DINO self-supervised model has illuminated that features extracted from its deep layers exhibit noteworthy correlation patterns congruent with genuine semantic labels. This insight has spurred further investigation. Drawing from this, Hamilton et al. introduced STEGO, which employs contrastive loss to distill pre-trained unsupervised visual features into semantic clusters, thus achieving compelling unsupervised segmentation results \cite{hamilton2022unsupervised}. Furthermore, Melas-Kyriazi et al. leverage spectral clustering on deep unsupervised representations, achieving state-of-the-art performance while also capitalizing on the informative features derived from DINO's architecture. Further This growing body of work showcases the potential of self-supervised methods, especially those rooted in DINO, to advance the landscape of unsupervised semantic segmentation \cite{melas2022deep}.
% \todo{see if you included all literature work for Vision Transformer for Semantic Segmentation}
% \todo{ add little bit about Hierarchical Vision Transformer for say swin, satrt with where all hierarchy in transformer have been explored and in semnatic segmentation where all it fanne dout well}
% \todo{Then in the the section say a very little about groupVit and that you would be moving on to that and our ork builds on that }

% \section{Visual Grouping}

% \section{Visual Language Pretraining}
% Vision-language pretraining has emerged as a powerful paradigm, driving advancements in the intersection of vision and language understanding. By jointly modeling visual and textual modalities, these pretrained models have revolutionized a range of vision-language tasks. Vision-language pretraining involves training a joint model on a vast corpus of image and text data. By learning from the paired visual and textual information, the model gains a comprehensive understanding of the cross-modal relationships between images and their associated captions or descriptions. 

% Explain the math and notation.